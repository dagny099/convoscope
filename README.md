# ğŸ§­ Convoscope

![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.x-FF4B4B.svg)
![Tests](https://img.shields.io/badge/tests-76%20passing-brightgreen)
![Coverage](https://img.shields.io/badge/coverage-95%25-green)
![Docs](https://img.shields.io/badge/docs-MkDocs%20Material-blueviolet)
![License](https://img.shields.io/badge/license-MIT-lightgrey.svg)

**Convoscope** is a multi-provider AI chat interface built with **Streamlit**.  
It supports **OpenAI, Anthropic, and Google Gemini** models, provides **persistent conversation management**, and demonstrates **production-grade engineering practices** for LLM applications.

---

## ğŸš€ App in Action

ğŸ“¸ *[Insert screenshot or animated GIF of a conversation here]*

Showcases:
- Real-time streaming responses
- Provider switching
- Conversation history and export

---

## âš¡ Quick Start

Clone the repo and launch in minutes.

```bash
git clone https://github.com/dagny099/convoscope.git
cd convoscope
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env        # add your API keys
```

Run the app:

```bash
streamlit run run_chat.py
```

ğŸ‘‰ Visit: [http://localhost:8501](http://localhost:8501)

---

## âœ¨ Features at a Glance

- ğŸ”„ **Multi-Provider LLMs** â€” OpenAI, Anthropic, and Gemini with automatic fallback  
- ğŸ’¾ **Conversation Management** â€” save, reload, and export sessions  
- ğŸ›¡ï¸ **Production-Ready Reliability** â€” input validation, logging, error recovery, rate-limit handling  
- ğŸ¨ **Polished UI/UX** â€” responsive layout, dark/light mode, provider status indicators  
- ğŸ§ª **Robust Testing** â€” 50+ unit tests, 20+ integration tests (Playwright + pytest)  
- ğŸ“‘ **Extensive Documentation** â€” MkDocs site with diagrams, metrics, and examples  

---

## ğŸ—ï¸ High-Level Architecture

_Provider abstraction with intelligent fallback; UI and session separated from services and storage._

```mermaid
flowchart TB
    subgraph Frontend ["ğŸ¨ Frontend Layer"]
        UI["ğŸ“± Streamlit UI"]
        SESSION["ğŸ“‹ Session Management"]
    end

    subgraph Services ["âš™ï¸ Service Layer"]
        LLM["ğŸ¤– LLM Service"]
        CONV["ğŸ’¬ Conversation Manager"]
        ERROR["âš ï¸ Error Handler"]
    end

    subgraph Storage ["ğŸ’¾ Storage Layer"]
        FILES["ğŸ“ File Storage"]
        CONVDB["ğŸ“Š Conversation Data"]
    end

    subgraph External ["ğŸŒ External APIs"]
        OPENAI["ğŸ”¥ OpenAI API"]
        ANTHROPIC["ğŸ§  Anthropic API"]
        GOOGLE["ğŸŒŸ Google Gemini"]
    end

    UI <--> SESSION
    UI --> LLM
    UI --> CONV

    LLM <--> ERROR
    LLM --> OPENAI
    LLM --> ANTHROPIC
    LLM --> GOOGLE

    CONV --> FILES
    CONV --> CONVDB
    SESSION --> CONV

    classDef frontend fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef services fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef storage fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef external fill:#e8f5e8,stroke:#388e3c,stroke-width:2px

    class UI,SESSION frontend
    class LLM,CONV,ERROR services
    class FILES,CONVDB storage
    class OPENAI,ANTHROPIC,GOOGLE external
```

â¡ï¸ Deeper dive: **[Architecture Docs](docs/architecture.md)**

---

## ğŸ”„ End-to-End Data Flow

_Validation â†’ routing â†’ streaming â†’ persistence â†’ recovery._

```mermaid
flowchart TD
    A[ğŸ§‘â€ğŸ’» User Input] --> B[ğŸ” Input Validation]
    B --> |Valid| C[ğŸ§  Session State Check]
    B --> |Invalid| D[âŒ User-Friendly Error]

    C --> E[ğŸ¤– LLM Router]
    E --> F{Provider Available?}

    F --> |OpenAI| G[ğŸ”¥ OpenAI API]
    F --> |OpenAI Failed| H[ğŸ§  Anthropic Fallback]
    F --> |All Failed| I[ğŸš¨ Error Handler]

    G --> J[ğŸ§µ Response Processing]
    H --> J
    I --> K[â— User Error Message]

    J --> L[ğŸ”Š Stream to UI]
    J --> M[ğŸ’¾ Save to Conversation Manager]

    M --> N[ğŸ“ File Storage]
    M --> O[ğŸ›Ÿ Auto-save Backup]

    L --> P[ğŸ“º Display to User]
    N --> Q[ğŸ—‚ï¸ Conversation History]
    O --> R[ğŸ§¯ Recovery System]

    style A fill:#e1f5fe
    style P fill:#e8f5e8
    style K fill:#ffebee
    style I fill:#ffebee
```

â¡ï¸ Deeper dive: **[Data Flow Docs](docs/data-flow.md)**

---

## ğŸ§ª Testing & Quality

Run the test suite:

```bash
pytest tests/ -v
```

- 50+ **unit tests**  
- 20+ **integration tests** (Streamlit + Playwright)  
- Mocked LLMs for reproducibility  
- Coverage reports:  

```bash
pytest --cov=src --cov-report=html
```

---

## âš™ï¸ Configuration

Environment variables are stored in `.env`:

```ini
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
DEFAULT_LLM_PROVIDER=openai
DEFAULT_TEMPERATURE=0.7
MAX_CONVERSATION_HISTORY=100
```

Default provider/model priorities can be adjusted in `config.py`.

---

## ğŸ“š Documentation

Convoscope is documented with **MkDocs**:

- Architecture diagrams  
- API usage examples  
- Before/after metrics and benchmarks  

ğŸ‘‰ Explore the docs: [link being updated super shortly]

---

## ğŸ¯ For Hiring Managers

This repository highlights my strengths in:

- **Design & Architecture** â€” modular refactoring from monolith  
- **Testing & Quality Engineering** â€” robust unit and integration testing  
- **Technical Writing** â€” professional docs with diagrams & examples  
- **System Integration** â€” multi-provider, resilient chat app design  

For more about my work, visit [my portfolio](https://barbhs.com).

---

## ğŸ”® Want the Backstory?

Convoscope started life as a 696-line monolith. Over time, I refactored it into a modular, testable, production-grade system.  

Iâ€™m writing a blog series about this journey:  
- Part 1: From Monolith to Modules  
- Part 2: Testing as a First-Class Citizen  
- Part 3: Fallbacks, Reliability, and UX Polish  

Stay tuned ğŸ‘€

---

## ğŸ“œ License

MIT â€” free to use, adapt, and explore.
